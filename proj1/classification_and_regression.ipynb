{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Fall 2020\n",
    "\n",
    "CS343: Neural Networks\n",
    "\n",
    "Project 1: Single layer networks\n",
    "\n",
    "**Submission reminders**:\n",
    "- Submit rubric on Google Classroom\n",
    "- Submit one .zip file per team on Google Classroom. Includes:\n",
    "    - All .ipynb notebook files\n",
    "    - All .py code files\n",
    "    - Data files under 10 MB\n",
    "- Did you answer all 15 questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from adaline import Adaline\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def plot_adaline_train(net, loss_list, acc_list, plotMarkers=False):\n",
    "    n_epochs = len(loss_list)\n",
    "    \n",
    "    x = np.arange(1, n_epochs+1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "    fig.suptitle(f'ADALINE ({n_epochs} epochs)')\n",
    "    \n",
    "    curveStr = '-r'\n",
    "    if plotMarkers:\n",
    "        curveStr += 'o'\n",
    "    \n",
    "    ax1.plot(x, loss_list, curveStr)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss (Sum squared error)')\n",
    "    ax2.plot(x, acc_list, curveStr)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paste in your code to load Old Faithful data with standardized features below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: ADALINE for regression\n",
    "\n",
    "Given ADALINE's linear (identity) activation and sum-of-squares loss function, the learned weights can be used for more than just classification. In this task, you will use ADALINE to perform a linear regression (*the same neural network subsumes what you did in CS251!*).\n",
    "\n",
    "### Goal\n",
    "\n",
    "Your goal is to get an ADALINE network to predict `waiting` from `eruptions`. That is, you are setting up a simple (bivariate) linear regression with equation $$y_i = m \\times x_i + b$$where the \"x\" variable (*predictor variable*) is `eruptions` and the \"y\" variable (*response variable*) is `waiting` (raw).\n",
    "\n",
    "**Ultimately, you want to draw a regression line *through* the Old Faithful data clusters to *join* rather than divide them.**\n",
    "\n",
    "### Design\n",
    "\n",
    "You can do the regression with the exact network you have currently implemented. **You shouldn't make any code changes to your `Adaline` class.**  In the cell below, use your network to set up the regression by making appropriate design choices:\n",
    "- Network input features: How many? What should they be?\n",
    "- Weights: What do they mean in this problem context?\n",
    "- What are the \"classes\"?\n",
    "\n",
    "### Tips\n",
    "\n",
    "- I suggest using the standardized version of the predictor (otherwise you may run into numeric stability issues), but it's fine to use the raw/unstandardized response variable. \n",
    "- Default hyperparameters should work well.\n",
    "- You may need to add a singleton dimension below so that your existing code works i.e. `shape=(272,1)`, NOT `shape=(272,)`\n",
    "\n",
    "**Write your training code in the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, **create a scatter plot of the data and the overlayed regression line**. Have the x-axis map onto standardized `eruptions` and y-axis onto raw `waiting` values.\n",
    "\n",
    "### Tips\n",
    "- You will need to leverage the model linear equation to go from x values to predicted y values. $y_i = m \\times x_i + b$\n",
    "- Look at the class boundary plot code that you used for classification. You will need to generate linearly spaced x values before plotting your regression y values on your regression line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: ADALINE and logistic regression\n",
    "\n",
    "In this task, you will extend ADALINE to logistic regression, where we explicitly represent the probability of class membership.\n",
    "\n",
    "For example data point $i$ is 80% likely to be in class A and 20% in class B.\n",
    "\n",
    "**Remember:** Despite the name, logistic regression is actually about solving a **classification** problem. So this is more similar to Task 2 than Task 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Implement logistic regression\n",
    "\n",
    "Create a subclass of `Adaline` called `AdalineLogistic` in a new file called `adaline_logistic.py`. **Only override existing methods as needed to make the following changes. DO NOT MODIFY `adaline.py` FOR ANY REASON!!**.\n",
    "\n",
    "#### Design\n",
    "\n",
    "1. Use the sigmoid activation function. $z = f(x) = \\frac{1}{1+e^{-x}}$\n",
    "2. Represent the output classes as 0 or +1. This should require a code change (activation values >=0.5 are classified as 1, otherwise class 0) and preprocessing of the old faithful data.\n",
    "3. Use the cross-entropy loss function: $\\sum_{i=1}^n \\left [ -y_i Log(z_i) - (1-y_i)Log(1 - z_i) \\right ] $\n",
    "where $z_i$ is the activation to input sample $i$ and $y_i$ is the corresponding $i^{th}$ class label (0 or 1).\n",
    "\n",
    "\n",
    "#### Todo below:\n",
    "\n",
    "- Train your network using the standardized Old Faithful data. Default hyperparameters should work fine.\n",
    "- Plot your loss and accuracy as a function of epoch.\n",
    "- Plot the logistic regression decision boundary and the data (Use your code from Task 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 11.** Why do we need to relabel the classes from -1/+1 to 0/1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "11. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaline_logistic import AdalineLogistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Test point probabilities\n",
    "\n",
    "#### Questions\n",
    "\n",
    "**Question 12.** Determine the probability that the following test points belong to **either class**:\n",
    "\n",
    "Format: standardized (eruptions, waiting)\n",
    "- (0.4, 0.98)\n",
    "- (0.5, -2)\n",
    "- (-1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "12. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Network classification performance comparison\n",
    "\n",
    "In this task, you will compare the performance of the two classification networks that you have built, plus a new one called Perceptron, on a larger dataset ([Ionosphere dataset](https://archive.ics.uci.edu/ml/datasets/Ionosphere)). A **Perceptron** is a single-layer neural network that works exactly the same as ADALINE, except it uses a different network activation function (`netAct`). The activation function computes the `netAct` as follows:\n",
    "\n",
    "$$\\text{netAct}_i = f(\\text{netIn}_i) = 1  \\text{ if netIn}_i \\geq 0$$\n",
    "$$\\text{netAct}_i = f(\\text{netIn}_i) = -1 \\text{ if netIn}_i < 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Create a Perceptron network and test it on Old Faithful data\n",
    "\n",
    "- Create a new class in `adaline.py` that will represent your Perceptron classifier. It should inherit from `Adaline`. Override/write any necessary functions. **Hint:** This should be really quick, short, and simple.\n",
    "- Use the `plot_nets_train` function below to create a \"1x2\" row of plots that resembles `plot_adaline_train`, where the left plot shows the loss over epochs and the right plot shows the accuracy over epochs. In each of these two plots, you should have three curves: one for ADALINE, one for logistic regression, one for the Perceptron.\n",
    "\n",
    "If your Perceptron is working, you should see similar (but not identical) plots to those produced by ADALINE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nets_train(nets, loss_lists, acc_lists, net_names, plotMarkers=False):\n",
    "    '''Creates a 1x2 grid of plots showing loss over epochs (left column) and\n",
    "    accuracy over epochs (right column) for one or more network (num_nets in total).\n",
    "    Generalizes `plot_adaline_train` for multiple trained networks.\n",
    "    \n",
    "    For example, in the case of two networks (e.g. adaline and perceptron; num_nets=2),\n",
    "    there would be two curves in each of the two plots.\n",
    "    \n",
    "    Put differently, the following function call would produce the same pair of plots you've\n",
    "    been getting up until this point with a single adaline network:\n",
    "        plot_adaline_train(nets[0], loss_lists[0], acc_lists[0], plotMarkers=plotMarkers)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nets: Python list of network objects. len(nets) = num_nets.\n",
    "    loss_lists: Python lists of ndarrays. len(loss_lists) = num_nets. len(loss_lists[0]) = n_epochs.\n",
    "        This would be a list of the loss histories for each of the nets being plotted.\n",
    "    acc_lists: Python lists of ndarrays. len(acc_lists) = num_nets. len(acc_lists[0]) = n_epochs.\n",
    "        This would be a list of the accuracy histories for each of the nets being plotted.\n",
    "    net_names: Python list of str. len(net_names) = num_nets.\n",
    "        Identifying names of each net (e.g. for legend).\n",
    "    plotMarkers: boolean.\n",
    "        Should we draw a plot marker at each epoch on each curve?\n",
    "    '''\n",
    "    n_nets = len(nets)\n",
    "    n_epochs = len(loss_lists[0])\n",
    "    \n",
    "    colors = ['orange', 'blue', 'red']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "    fig.suptitle(f'{n_nets} networks trained for ({n_epochs} epochs)')\n",
    "    \n",
    "    for net, loss_list, acc_list, color in zip(nets, loss_lists, acc_lists, colors):\n",
    "        x = np.arange(1, n_epochs+1)\n",
    "\n",
    "        curveStr = '-r'\n",
    "        if plotMarkers:\n",
    "            curveStr += 'o'\n",
    "\n",
    "        ax1.plot(x, loss_list, curveStr, c=color)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss (Sum squared error)')\n",
    "        ax2.plot(x, acc_list, curveStr, c=color)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "    \n",
    "    plt.legend(net_names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaline import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. The Ionosphere dataset\n",
    "\n",
    "The [Ionosphere dataset](https://archive.ics.uci.edu/ml/datasets/Ionosphere) is radar signal data collected in Goose Bay, Labrador. It is a more complex dataset than old faithful, with 33 features (but still 2 classes). The class values are coded 'g' for good radar signal and 'b' for bad radar signal.\n",
    "\n",
    "**Please download the CSV file from the CS343 project website (not above UCI link)** — I have slightly modified the dataset for your convenience.\n",
    "\n",
    "####  Comparing classifier performance\n",
    "\n",
    "Your goal is to train, compare, and analyze the performance of your ADALINE, logistic regression, and Perceptron networks on the Ionosphere dataset. How you do this is up to you (including hyperparameters), but you should include the following two plots and answer the below questions.\n",
    "- One 1x2 plot showing training loss and accuracy of the three networks.\n",
    "- Two scatter plots (one per network) showing the feature at index 4 on x-axis and the feature at index 19 on the y-axis. Samples should be color coded one of two colors: whether the class was correctly or incorrectly predicted. The data should be plotted on their original scale (i.e. not normalized).\n",
    "\n",
    "I encourage you to play with the hyperparameters, but in the final plots that you turn-in, please use the same hyperparameter values across the networks to show a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 13.** What final accuracy are you able to achieve on the Ionosphere dataset with the three networks?\n",
    "\n",
    "**Question 14.** Play with the hyperparameters a bit.<br/>(a) For the network that achieves the best final epoch accuracy, what is its key advantage (other than better final accuracy)?<br/>(b) What advantage do the runner-up networks have over the winner, based on the training plots? Explain.\n",
    "\n",
    "**Question 15.** In your scatter plots showing correct and incorrect sample classifications, how is it possible that misclassifications do not appear to either side of a simple boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "13. \n",
    "\n",
    "14. \n",
    "\n",
    "15. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**NOTE:** Never integrate extensions into your base project so that it changes the expected behavior of core functions. If your extension changes the core design/behavior, no problem, duplicate your working base project and add features from there.\n",
    "\n",
    "Generally, a small number of \"in-depth\" extensions count for more than many \"shallow\" extensions.\n",
    "\n",
    "1. Extend the ADALINE model to multi-class classification using the One-Vs-Rest (OvR) method. Recall that with this scheme, we train multipler networks with each of the $n$ output classes serving as the +1 class (others set to 0 class). For example, for classes [a, b, c] would would train the networks with the following class labels: [1, 0, 0], [0, 1, 0], [0, 0, 1], respectively. We then classify based on the class that generates the highest max probability / activation value. Test it on a dataset with more than two classes (e.g. Iris). \n",
    "2. Create plots of the ADALINE regression after training on different numbers of epochs. One options is to plot all the curves in a single plot and establish a color scheme for time so that the viewer can visually discern the time sequence. Another possibility is to create a NxM grid of plots showing the progression (be sure to label the titles with #epochs).\n",
    "3. Demonstrate how ADALINE can handle multiple linear regression.\n",
    "4. Test the performance of single layer neural networks at classifying a binary class dataset of your choice.\n",
    "5. Compare the performance of ADALINE, Perceptron, and Logistic Regression single-layer networks in additional ways and/or with additional datasets.\n",
    "6. (Small) Make your early stopping implementation fancier. For example, only stop if the change relative to the average loss over the most recent few epochs is less than the tolerance. Why could this be an improvement over the other method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
