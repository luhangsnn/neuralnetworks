{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Luhang Sun & Roujia Zhong**\n",
    "\n",
    "Fall 2020\n",
    "\n",
    "CS343: Neural Networks\n",
    "\n",
    "Project 1: Single-layer networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from adaline import Adaline\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "In this project, you will implement single-layer neural networks that includes the same fundamental components as larger multi-level networks.\n",
    "\n",
    "**The goals are to:**\n",
    "\n",
    "- Get familar with Jupyter notebooks, Numpy arrays, Pandas.\n",
    "- Get familar with the workflow for preprocessing data, training a neural network, evaluating test data, and examining performance metrics.\n",
    "- Analyze and visualize the decision boundaries formed by single-layer networks.\n",
    "- Use a similar neural network architecture for classification and for regression.\n",
    "- Work with multiple datasets.\n",
    "\n",
    "### Reminders\n",
    "\n",
    "- In this class, use `numpy ndarray` (`np.array()`), not Numpy Matrix.\n",
    "- To help safeguard against data loss when working in a jupyter notebook, make sure the notebook is `Trusted` (Top right corner of notebook). This will ensure your work autosaves. **I still recommend manually saving at least every few minutes with (Control+S / Cmd+S)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement the ADAptive LInear NEuron (ADALINE) network for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task 1, complete the methods of the `Adaline` class in `adaline.py`. This includes:\n",
    "\n",
    "- `net_input(self, features)`\n",
    "- `activation(self, net_in)`\n",
    "- `compute_loss(self, errors)`\n",
    "- `compute_accuracy(self, y, y_pred)`\n",
    "- `gradient(self, errors, features)`\n",
    "\n",
    "- `predict(self, features)`\n",
    "- `fit(self, features, y, n_epochs, lr)`\n",
    "\n",
    "**Important:** Before starting, read through the method descriptions and expected inputs/outputs. It probabily woud be a good idea to tackle simpler/smaller methods first, then use them in more complex ones. For example, it may be a good idea to work on `net_input` first because it is required to complete `fit`. There is test code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of the ADALINE network equations\n",
    "\n",
    "##### Net input\n",
    "\n",
    "$\\vec{x}_i$ is one of the $N$ data sample vectors from the dataset matrix $A$. That is, `x_i.shape = (M,)`.\n",
    "\n",
    "$$\\text{netIn}_i = \\sum_{j=1}^M x_{ij} w_j + b$$\n",
    "\n",
    "##### Net activation\n",
    "\n",
    "$$f(z_i) = z_i$$\n",
    "\n",
    "##### Loss: Sum of squared error\n",
    "\n",
    "$$L(\\vec{w}) = \\frac{1}{2} \\sum_{i=1}^N \\left ( y_i - \\text{netAct}_i \\right )^2 $$\n",
    "\n",
    "##### Gradient (bias)\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^N \\left ( y_i - \\text{netAct}_i \\right )$$\n",
    "\n",
    "##### Gradient (wts)\n",
    "\n",
    "Below, $x_{ij}$ is the $j^{th}$ feature of the data sample vector $\\vec{x}_i$.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_j} = -\\sum_{i=1}^N \\left ( y_i - \\text{netAct}_i \\right ) x_{ij}$$\n",
    "\n",
    "##### Gradient descent (delta rule)\n",
    "\n",
    "$$b(t+1) = b(t) - \\eta \\frac{\\partial L}{\\partial b}$$\n",
    "$$w_j(t+1) = w_j(t) - \\eta \\frac{\\partial L}{\\partial w_j}$$\n",
    "\n",
    "above $\\eta$ is the learning rate, and $N$ is the training set (number of data samples in training epoch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Test your ADALINE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Adaline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `loss` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Your loss is 3.6609344768925496 and it should be 3.6609344768925496\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "randErrors = np.array([-0.835,  0.322, -0.381,  0.496, -0.89 , -0.953])\n",
    "net_act = np.random.rand(len(randErrors))\n",
    "debugLoss = net.compute_loss(randErrors, net_act)\n",
    "print(f'Your loss is {debugLoss} and it should be 3.6609344768925496')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `accuracy` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test 1: Your accuracy is 1.0 and it should be 1.0\nTest 2: Your accuracy is 0.33333333333333337 and it should be 0.33333333333333337\n"
    }
   ],
   "source": [
    "randClasses1 = np.where(randErrors >= 0, 1, -1)\n",
    "randClasses2 = np.roll(randClasses1, 1)\n",
    "acc1 = net.compute_accuracy(randClasses1, randClasses1)\n",
    "acc2 = net.compute_accuracy(randClasses1, randClasses2)\n",
    "print(f'Test 1: Your accuracy is {acc1} and it should be 1.0')\n",
    "print(f'Test 2: Your accuracy is {acc2} and it should be 0.33333333333333337')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(10, 3) (10,)\nTest 1: Your bias gradient is 0.7839944892482784 and it should be 0.7839944892482784\nTest 2: Your wt gradient is [ 0.4   -0.897  7.689] and it should be [ 0.4   -0.897  7.689]\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "randFeatures = np.random.normal(loc=0, scale=1, size=(10,3))\n",
    "randErrors1 = np.random.normal(loc=0, scale=1, size=(10,))\n",
    "randBiasGrad, randWtGrad = net.gradient(randErrors1, randFeatures)\n",
    "print(f'Test 1: Your bias gradient is {randBiasGrad} and it should be 0.7839944892482784')\n",
    "print(f'Test 2: Your wt gradient is {randWtGrad} and it should be [ 0.4   -0.897  7.689]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `predict` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Your predicted classes are [-1 -1 -1 -1 -1 -1 -1 -1  1 -1].\n            They should be [-1 -1 -1 -1 -1 -1 -1 -1  1 -1]\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "inputs = np.random.randn(10, 5)\n",
    "net.wts = np.random.randn(6)\n",
    "y_pred = net.predict(inputs)\n",
    "print(f'Your predicted classes are {y_pred}.\\n            They should be [-1 -1 -1 -1 -1 -1 -1 -1  1 -1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Your end-of-training loss / accuracy are\n3.9448748875503834 / 0.6.\nThey should be\n3.944874887550384 / 0.6\nYour wts after training are:\n[-0.298 -0.033  0.33  -0.382 -0.192  0.087]\nand should be\n[-0.298 -0.033  0.33  -0.382 -0.192  0.087]\n"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "net = Adaline()\n",
    "inputs = np.random.randn(10, 5)\n",
    "y = np.sign(np.random.randn(10))\n",
    "loss, acc = net.fit(inputs, y)\n",
    "print(f'Your end-of-training loss / accuracy are\\n{loss[-1]} / {acc[-1]}.\\nThey should be\\n3.944874887550384 / 0.6')\n",
    "print(f'Your wts after training are:\\n{net.get_wts()}\\nand should be\\n[-0.298 -0.033  0.33  -0.382 -0.192  0.087]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Load in and preprocess old faithful data\n",
    "\n",
    "Write code to do the following in the below cell.\n",
    "\n",
    "- Load in `old_faithful.csv`, represent data using a ndarray. Shape = [Num samps, Num features] = [272, 2].\n",
    "- Assign the output classes (**severe**) to a separate 1D ndarray vector. Shape=(272,)\n",
    "- Preprocess the data by normalizing by the global maximum. I.e. the max value over all features in the transformed dataset should be 1.0. Stated a different way, only one feature component of one data sample vector equals 1.0, everything else is smaller, but still $>=$ 0.\n",
    "- Use matplotlib to create a scatter plot of the normalized data, color-coding data points according to their class\n",
    "- I suggest using pandas, but you're welcome to do this however you like.\n",
    "- **Make sure that executing the below cell results in an inline scatter plot, color-coded by class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Train ADALINE on normalized Old Faithful data using default hyperparameters (i.e. learning rate, epochs)\n",
    "\n",
    "Print out the final loss and accuracy, then use the provided function to plot your training results inline in the below cell.\n",
    "\n",
    "By the final epoch, training loss should reach ~23.74 and accuracy ~97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adaline_train(net, loss_list, acc_list, plotMarkers=False):\n",
    "    n_epochs = len(loss_list)\n",
    "    \n",
    "    x = np.arange(1, n_epochs+1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "    fig.suptitle(f'ADALINE ({n_epochs} epochs)')\n",
    "    \n",
    "    curveStr = '-r'\n",
    "    if plotMarkers:\n",
    "        curveStr += 'o'\n",
    "    \n",
    "    ax1.plot(x, loss_list, curveStr)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss (Sum squared error)')\n",
    "    ax2.plot(x, acc_list, curveStr)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    \n",
    "    plt.show()\n",
    "plot_adaline_train(net, loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 1.** Based on your loss and accuracy curves, does it look like your network learned to classify the old faithful data? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Early stopping\n",
    "\n",
    "A common practice to speed up training is to stop training early (before the prescribed number of epochs is completed) if the loss stops changing (within some criterion level). You will modify the `fit` training function to implement this feature.\n",
    "\n",
    "Add two optional parameters to `fit`:\n",
    "- `early_stopping=False`\n",
    "- `loss_tol=0.1`\n",
    "\n",
    "In this subtask, implement the following:\n",
    "- if `early_stopping` is True, terminate the training process if the difference in loss between the previous and current epoch is `< loss_tol`. This means that the loss has converged before the pre-specified number of training epochs.\n",
    "- Set the number of training epochs to be large (2000). Determine the number of epochs required for the loss to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Adaline()\n",
    "loss, acc = net.fit_early_stopping(x, y, n_epochs=2000, early_stopping=True, loss_tol=0.1)\n",
    "print(loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 2.** How many epochs did it take to train the network with the early stopping tolerance of 0.1?\n",
    "\n",
    "**Question 3.** At what approximate loss value does the network converge to when stopping early?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature scaling and convergence\n",
    "\n",
    "**Important:** For this task, disable early stopping  in `fit`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Feature scaling\n",
    "\n",
    "Copy your code from Task 1 to import the Old Faithful data, but this time don't normalize before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 4.** What happens to the loss when we don't normalize the features before training? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Test how individually standardizing your features affects the rate at which loss decreases over epochs\n",
    "\n",
    "- Write code in the cell below to train the network on standardized features. Recall that standardizing a variable means applying the transformation $\\frac{x - \\mu}{\\sigma}$. The mean and standard deviation should be computed over the entire dataset and separately per feature.\n",
    "- Plot the loss and accuracy.\n",
    "\n",
    "**The cell should generate an inline pair of plots when executed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 5.** Explain the similarities/differences in loss and accuracy curves between these plots and those that you made in Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Visualize class boundaries\n",
    "\n",
    "For this subtask, you will plot the boundary between points (`eruptions`, `waiting` feature pairs) that get classified as severe (+1) or not (-1). To get there, fill in the blanks and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print your learned wts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 6.** What is the meaning of each of the above learned weights with respect to the variables/features in the dataset? Hint: Look at your `net_in` equation, look at the features that you feed into the model, look at the scatterplot you made in 1b, think about what features are present in a single training sample.\n",
    "\n",
    "**Question 7.** Which feature / weight index corresponds to the \"y axis value\" in your scatterplot from 1b?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "6. \n",
    "7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform weights for plotting\n",
    "\n",
    "The class boundary equation is $0 = w_0 + w_1 \\times x_i + w_2 \\times y_i$ for sample $i$ in our data ($i$ goes to 272). But to plot it, we need an equation that looks like $y_i = m \\times x_i + b$ where $m$ and $b$ are some combinations of our weights.\n",
    "\n",
    "- Scale the weights so that the one corresponding to the \"y value\" is set to 1, then solve for $y$ (*It might be helpful to work this out by hand*). Once you do, adjust the sign/scale of your weights in code so they match up with the equation you wrote out by hand ( of form $y_i = m \\times x_i + b$).\n",
    "- Once you're done, have the cell below print your transformed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the cell below, sample 50 equally spaced x values from -1.5 to 1.5 for plotting the class boundary. Given the `x_i` values, generate `y_i` values using the equation $y_i = m \\times x_i + b$ (using your transformed weights from above). \n",
    "\n",
    "**Executing the code below should produce a graph that clearly shows this class boundary superimposed on your data scatter plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Hyperparameters and grid search\n",
    "\n",
    "This task focuses on the influence of learning rate (a model **hyperparameter**) on the quality of neural network training.\n",
    "\n",
    "For this task, use the standardized Old Faithful features for input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Influence of learning rate on learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Adaline()\n",
    "loss, acc = net.fit(x, y, lr=0.0001, n_epochs=10)\n",
    "plot_adaline_train(net, loss, acc, plotMarkers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 8.** Make small changes to the learning rate hyperparameter above. How does it affect the loss?\n",
    "\n",
    "**Question 9.** What happens if the learning rate is increased by several orders of magnitude? How does it affect the loss? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "8. \n",
    "9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Grid search\n",
    "\n",
    "How can we determine a \"good\" value of the learning rate that doesn't result in pathological training behavior? Before, we were just guessing / eye-balling it. \n",
    "\n",
    "One common, simple approach is to perform a **grid search** through the parameter space. We select with a lower bound value for the parameter and walk through the parameter space in equal steps until we hit a upper bound value. After the search concludes, we set the parameter value for \"real simulations\" to the value that \"worked best\" according to some criteria we're interested in optimizing during the grid search.\n",
    "\n",
    "#### Todo\n",
    "\n",
    "Use grid search to find the highest learning rate that still lowers loss as a function of epoch (i.e. below this value, the loss decreases as a function of epoch, above this critical value, loss increases as a function of epoch). Find the value with precision of at least 1e-3 (10^-3). **Write all your code below and the learning rate that you find in the cell below** â€” do NOT modify the `Adaline` class.\n",
    "\n",
    "**Tips:**\n",
    "- Define lower and upper bound values for the parameter search, informed by your explorations in 3a.\n",
    "- Define a step size at least as small as your starting value. Your simulation shouldn't last more a few seconds.\n",
    "- Keep the number of epochs small (e.g. 10) to keep simulation time reasonable\n",
    "- Remember that you are outputting the entire loss history when you execute `fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 10.** What is the critical learning rate value above which loss starts to increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}